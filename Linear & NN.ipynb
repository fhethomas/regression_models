{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models\n",
    "This file is for test and creation of linear/logistic regression models\n",
    "1. Linear_regression built - testing to be done\n",
    "2. Logistic_regression to be built as a class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " feature scaling occurs across categories i.e. we're training a model\n",
    " to recognise dogs, we normalise across:\n",
    " tail length, leg length, nose-pointy-outness\n",
    "                  normalisation (x-xmin)/(xmax-xmin)\n",
    "                 standardisation (X-Xsample_mean)/Xsample_std\n",
    "\"\"\"\n",
    "def normalize(X):\n",
    "    # normalize across features\n",
    "    m,n = X.shape\n",
    "    for col in range(n):\n",
    "        X[:,col] = ((X[:,col]-np.min(X[:,col]))/(np.max(X[:,col])-np.min(X[:,col])))\n",
    "    return X\n",
    "def standardize(X):\n",
    "    # standardize across features\n",
    "    m,n = X.shape\n",
    "    for col in range(n):\n",
    "        X[:,col] = (X[:,col]-np.mean(X[:,col]))/np.std(X[:,col])\n",
    "    return X\n",
    "# Creation of linear regression class:\n",
    "class Linear_regression:\n",
    "    \"\"\"Linear regression model\n",
    "    Parameters\n",
    "    -----------\n",
    "    alpha : float, default 0.01\n",
    "        learning rate of gradient descent\n",
    "    degree_accuracy : float, default 0.05\n",
    "        degree of accuracy that linear\n",
    "        regression is looking for during \n",
    "        gradient descent\n",
    "    feature_scaling : string, default None\n",
    "        options: None,'normalize'\n",
    "        \"\"\"\n",
    "    def __init__(self,alpha=0.01, degree_accuracy=0.05,feature_scaling=None):\n",
    "        self.alpha = alpha\n",
    "        self.degree_accuracy = degree_accuracy\n",
    "        self.thetas_set = False\n",
    "        self.feature_scaling_options = [None,'normalize']\n",
    "        assert feature_scaling in self.feature_scaling_options,\"no such feature scaling option\"\n",
    "        self.feature_scaling = feature_scaling\n",
    "        self.theta = np.array([])\n",
    "    def generate_theta(self,X):\n",
    "        self.theta = np.random.rand(X.shape[1],1)\n",
    "    def add_intercept(self,X):\n",
    "        return np.concatenate((X,np.ones((X.shape[0],1))),axis=1)\n",
    "    def computeCost(self,X,y,theta):\n",
    "        # Cost function\n",
    "        m,n=X.shape\n",
    "        J=0\n",
    "        X=(np.dot(X,theta))\n",
    "        J=sum(np.power(X-y,2))\n",
    "        J*=(1.0/(2.0*m))\n",
    "        return J\n",
    "    def coefficients(self):\n",
    "        # prints coefficients\n",
    "        print(self.theta[:-1,:])\n",
    "    def intercept(self):\n",
    "        # print intercept\n",
    "        print(self.theta[-1,:])\n",
    "    def fit(self,X,y):\n",
    "        \"\"\"fits model\n",
    "        Parameters\n",
    "        --------------\n",
    "        X : numpy array\n",
    "            Array should be independent variables.\n",
    "            Shape must be m * n, where m is cases\n",
    "            and n is features\n",
    "        y : numpy array\n",
    "            Array should be dependent variable\n",
    "            Shape should be m * 1, where m is cases\n",
    "        Returns\n",
    "        --------------\n",
    "        theta : numpy array\n",
    "            These are the coefficients of the linear\n",
    "            regression. This will be stored in the \n",
    "            model for use in predict\"\"\"\n",
    "        \n",
    "        alpha=self.alpha\n",
    "        degree_accuracy=self.degree_accuracy\n",
    "        if self.thetas_set == False:\n",
    "            if self.feature_scaling == 'normalize':\n",
    "                X=normalize(X)\n",
    "            X = self.add_intercept(X)\n",
    "            self.generate_theta(X)\n",
    "            self.thetas_set = True\n",
    "        theta = self.theta\n",
    "        h=0\n",
    "        m,n=X.shape\n",
    "        J_history=[]\n",
    "        J=999.0\n",
    "        iterations=0\n",
    "        print(theta)\n",
    "        while J>=degree_accuracy:\n",
    "            h=np.dot(X,theta)\n",
    "            #print('h1: {0}'.format(h))\n",
    "            h=np.dot(X.T,h-y)/float(m)\n",
    "            #print('h2: {0}'.format(h))\n",
    "            theta=theta-(h*alpha)\n",
    "            #print(theta)\n",
    "            J=self.computeCost(X,y,theta)\n",
    "            J_history.append(J)\n",
    "            iterations+=1\n",
    "            if iterations > 2 and J > J_history[-2]:\n",
    "                print('J increasing, reducing alpha to: {0}'.format(alpha/2))\n",
    "                self.generate_theta(X)\n",
    "                self.alpha = alpha/2\n",
    "                theta = self.fit(X,y)\n",
    "                return theta\n",
    "        #print(J_history)\n",
    "        print('Returned Thetas are: {0}'.format(theta))\n",
    "        print('degree of accuracy: %s' % J_history[-1])\n",
    "        print('number of iterations: %s' % iterations)\n",
    "        self.theta = theta\n",
    "        return theta\n",
    "    def predict(self,X):\n",
    "        \"\"\"Use this function to predict y\n",
    "        from a set of X data\n",
    "        Parameters\n",
    "        ------------\n",
    "        X : numpy array\n",
    "            independent variables, shaped\n",
    "            m * n, where m is case and n is \n",
    "            feature\n",
    "        \"\"\"\n",
    "        if self.feature_scaling == 'normalize':\n",
    "            X=normalize(X)\n",
    "        X = self.add_intercept(X)\n",
    "        return np.dot(X,self.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Step 1 generate FAKE data\n",
    "# These are the test thetas being used to generate fake data\n",
    "theta0=13\n",
    "theta1=21\n",
    "X=np.random.randint(1,high=80,size=(90,1))\n",
    "X=X.reshape(len(X),1)\n",
    "#one=np.ones(X.shape)\n",
    "#X=np.concatenate((one,X),axis=1)\n",
    "y=np.array([[theta0+X[i,0]*theta1] for i in range(len(X))])\n",
    "\n",
    "# Step 2 - create and fit linear regression model\n",
    "lr=Linear_regression(feature_scaling=None)\n",
    "lr.fit(X,y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=np.array([np.nan,1,2,np.nan])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.isnan(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X=iris.data[:,:2]\n",
    "y=iris.target\n",
    "theta=np.random.rand(X.shape[1])\n",
    "X=X[y<2,:]\n",
    "y=y[y<2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "X=normalize(X)\n",
    "#cost_function(theta,X,y)\n",
    "#gradient(theta,X,y)\n",
    "res=minimize(cost_function,theta,args=(X,y),jac=gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "m=97\n",
    "alpha=0.01\n",
    "test_e=np.zeros((m,1))\n",
    "test_e+=np.exp(1)\n",
    "theta=np.zeros((3,1))\n",
    "y=np.zeros((m,1))\n",
    "one=np.ones((m,1))\n",
    "y+=0.01\n",
    "X=np.ones((m,3))\n",
    "for x in range(len(y)):\n",
    "    if x<45:\n",
    "        y[x]=0\n",
    "        X[x,1]= random.randint(1,6)\n",
    "        X[x,2]= random.random()*2\n",
    "        \n",
    "    else:\n",
    "        y[x]=1\n",
    "        X[x,1]= random.randint(5,11)\n",
    "        X[x,2]= 2+random.random()*7\n",
    "def cost_function(theta,X,y):\n",
    "    m,n=X.shape\n",
    "    theta = theta.reshape(n,1)\n",
    "    a=np.dot(y.T,np.log(g(X,theta)))\n",
    "    a[np.isnan(a)]=0\n",
    "    b=np.dot((one-y).T,np.log(1-g(X,theta)))\n",
    "    b[np.isnan(b)]=0\n",
    "    j=np.array((-1/float(m))*(a-b),dtype=float)\n",
    "    return np.absolute(j)\n",
    "def gradient_descent(alpha,theta,X,y,m):\n",
    "    theta=theta-(alpha/m)*np.dot(X.T,g(X,theta)-y)\n",
    "    return theta\n",
    "def gradient(theta,X,y):\n",
    "    m,n = X.shape\n",
    "    theta = theta.reshape(n,1)\n",
    "    return np.dot(X.T,g(X,theta)-y).flatten()\n",
    "def g(X,theta):\n",
    "    return 1/(1+np.power(test_e,-np.dot(X,theta)))\n",
    "for x in range(1000):\n",
    "    theta = gradient_descent(alpha,theta,X,y,m)\n",
    "#print(cost_function(X,y,theta))\n",
    "print(theta)\n",
    "#print(test_e)\n",
    "print(g(X,theta))\n",
    "#gradient_descent(alpha,theta,X,y,m)\n",
    "cost_function(theta,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_cg #fmin_cg to train neural network\n",
    "\n",
    "class Logistic_regression:\n",
    "    def __init__(self,alpha=0.01):\n",
    "        self.alpha=alpha\n",
    "        self.theta=np.array([])\n",
    "    def g(self,X,theta):\n",
    "        m,n=X.shape\n",
    "        test_e=np.zeros((m,1))\n",
    "        test_e+=np.exp(1)\n",
    "        print('checking:')\n",
    "        print(test_e.shape)\n",
    "        print(np.dot(X,theta).shape)\n",
    "        return 1/(1+np.power(test_e,-np.dot(X,theta)))\n",
    "    def cost_function(self,theta,X,y):\n",
    "        m,n=X.shape\n",
    "        one=np.ones((m,1))\n",
    "        theta = theta.reshape(n,1)\n",
    "        a=np.dot(y.T,np.log(self.g(X,theta)))\n",
    "        a[np.isnan(a)]=0\n",
    "        b=np.dot((one-y).T,np.log(1-self.g(X,theta)))\n",
    "        b[np.isnan(b)]=0\n",
    "        j=np.array((-1/float(m))*(a-b),dtype=float)\n",
    "        return np.absolute(j)\n",
    "    def gradient(self,theta,X,y):\n",
    "        m,n = X.shape\n",
    "        theta = theta.reshape(n,1)\n",
    "        return np.dot(X.T,self.g(X,theta)-y).flatten()\n",
    "    def fit(self,X,y):\n",
    "        m,n = X.shape\n",
    "        self.theta = np.random.randn(n)\n",
    "        theta = self.theta\n",
    "        self.theta = fmin_cg(self.cost_function,x0=theta,args=(X,y),fprime=self.gradient)\n",
    "        print('fitting complete')\n",
    "    def predict(self,X):\n",
    "        m,n = X.shape\n",
    "        theta = self.theta.reshape(n,1)\n",
    "        sig = self.g(X,theta)\n",
    "        sig[sig<0.5]=0\n",
    "        sig[sig>0.5]=1\n",
    "        return sig\n",
    "    def accuracy(self,X,y):\n",
    "        m,n = X.shape\n",
    "        theta = self.theta.reshape(n,1)\n",
    "        sig = self.g(X,theta)\n",
    "        sig[sig<0.5]=0\n",
    "        sig[sig>0.5]=1\n",
    "        return len(y[y==sig])/float(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n",
      "checking:\n",
      "(100, 1)\n",
      "(100, 1)\n",
      "checking:\n",
      "(100, 1)\n",
      "(100, 1)\n",
      "checking:\n",
      "(100, 1)\n",
      "(100, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-295e5ac5a9c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLogistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-99c978368022>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtheta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mtheta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtheta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfmin_cg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfprime\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fitting complete'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Frank\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36mfmin_cg\u001b[0;34m(f, x0, fprime, args, gtol, norm, epsilon, maxiter, full_output, disp, retall, callback)\u001b[0m\n\u001b[1;32m   1175\u001b[0m             'return_all': retall}\n\u001b[1;32m   1176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_minimize_cg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfprime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfull_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Frank\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m_minimize_cg\u001b[0;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, **unknown_options)\u001b[0m\n\u001b[1;32m   1245\u001b[0m             \u001b[0malpha_k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_fval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_old_fval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgfkp1\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m                      _line_search_wolfe12(f, myfprime, xk, pk, gfk, old_fval,\n\u001b[0;32m-> 1247\u001b[0;31m                                           old_old_fval, c2=0.4, amin=1e-100, amax=1e100)\n\u001b[0m\u001b[1;32m   1248\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0m_LineSearchError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m             \u001b[1;31m# Line search failed to find a better solution.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Frank\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m_line_search_wolfe12\u001b[0;34m(f, fprime, xk, pk, gfk, old_fval, old_old_fval, **kwargs)\u001b[0m\n\u001b[1;32m    763\u001b[0m     ret = line_search_wolfe1(f, fprime, xk, pk, gfk,\n\u001b[1;32m    764\u001b[0m                              \u001b[0mold_fval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_old_fval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m                              **kwargs)\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Frank\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\linesearch.py\u001b[0m in \u001b[0;36mline_search_wolfe1\u001b[0;34m(f, fprime, xk, pk, gfk, old_fval, old_old_fval, args, c1, c2, amax, amin, xtol)\u001b[0m\n\u001b[1;32m     99\u001b[0m     stp, fval, old_fval = scalar_search_wolfe1(\n\u001b[1;32m    100\u001b[0m             \u001b[0mphi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mderphi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_fval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_old_fval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mderphi0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             c1=c1, c2=c2, amax=amax, amin=amin, xtol=xtol)\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mstp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_fval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Frank\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\linesearch.py\u001b[0m in \u001b[0;36mscalar_search_wolfe1\u001b[0;34m(phi, derphi, phi0, old_phi0, derphi0, c1, c2, amax, amin, xtol)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mold_phi0\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mderphi0\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0malpha1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.01\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphi0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mold_phi0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mderphi0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0malpha1\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0malpha1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "lr=Logistic_regression(alpha=0.1)\n",
    "print(X.shape)\n",
    "lr.fit(X,y)\n",
    "lr.predict(X)\n",
    "lr.accuracy(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_cg #fmin_cg to train neural network\n",
    "class NeuralNetwork:\n",
    "    def __init__(self,hidden_layer_size=20,epsilon=0.12,lamb=1,feature_scaling=None):\n",
    "        self.hidden_layer_size=hidden_layer_size\n",
    "        self.epsilon=epsilon\n",
    "        self.lamb=lamb\n",
    "        self.X=np.array([[]])\n",
    "        self.y=np.array([[]])\n",
    "        self.theta=np.array([[]])\n",
    "        self.input_layer_size=5\n",
    "        self.output_layer_size=1\n",
    "        self.feature_scaling_options = [None,'normalize','standardize']\n",
    "        assert feature_scaling in self.feature_scaling_options,\"no such feature scaling option\"\n",
    "        self.feature_scaling=feature_scaling\n",
    "    def sigmoid(self,z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    def sigmoidGradient(self,z):\n",
    "        return np.multiply(self.sigmoid(z),(1-self.sigmoid(z)))\n",
    "    def initialise_thetas(self,input_layer_size,hidden_layer_size,output_layer_size):\n",
    "        theta1=np.random.rand(input_layer_size+1,hidden_layer_size)\n",
    "        theta2=np.random.rand(hidden_layer_size+1,output_layer_size)\n",
    "        theta=np.array([theta1,theta2])\n",
    "        theta=self.theta_flatten(theta)*2*self.epsilon-self.epsilon\n",
    "        return theta\n",
    "    def theta_flatten(self,theta):\n",
    "        theta_t=theta[:]\n",
    "        theta=np.array([])\n",
    "        #fmin_cg requires a gradient to be (m,0) dimensions\n",
    "        for x in theta_t:\n",
    "            theta=np.concatenate((theta,x.flatten()),0)\n",
    "        #theta=theta.reshape(len(theta),0)\n",
    "        #print(theta.dtype)\n",
    "        return theta\n",
    "    def theta_unflatten(self,theta,input_layer_size,hidden_layer_size,output_layer_size):\n",
    "        theta1=theta[:(input_layer_size+1)*hidden_layer_size].reshape((input_layer_size+1),hidden_layer_size)\n",
    "        theta2=theta[(input_layer_size+1)*hidden_layer_size:].reshape(hidden_layer_size+1,output_layer_size)\n",
    "        return theta1, theta2\n",
    "    def costFunction(self,theta,X,y,input_layer_size,hidden_layer_size,output_layer_size,lamb):\n",
    "        m,n=X.shape\n",
    "        theta1,theta2=self.theta_unflatten(theta,input_layer_size,hidden_layer_size,output_layer_size)\n",
    "        one=np.ones((m,1))\n",
    "        a1=np.concatenate((one,X),1)\n",
    "        a2=np.concatenate((one,self.sigmoid(np.dot(a1,theta1))),1)\n",
    "        sig=self.sigmoid(np.dot(a2,theta2))\n",
    "        cost=np.multiply(-y,np.log(sig))-np.multiply((1-y),np.log(1-sig))\n",
    "        theta1_bias=theta1[1:,:]\n",
    "        theta2_bias=theta2[1:,:]\n",
    "        J=(1/m)*sum(sum(cost))+(lamb/(2*m))*(sum(sum(np.square(theta1_bias)))+sum(sum(np.square(theta2_bias))))\n",
    "        return J\n",
    "    def nnGradient(self,theta,X,y,input_layer_size,hidden_layer_size,output_layer_size,lamb):\n",
    "        m,n=X.shape\n",
    "        theta1,theta2=self.theta_unflatten(theta,input_layer_size,hidden_layer_size,output_layer_size)\n",
    "        one=np.ones((m,1))\n",
    "        a1=np.concatenate((one,X),1)\n",
    "        a2=np.concatenate((one,self.sigmoid(np.dot(a1,theta1))),1)\n",
    "        sig=self.sigmoid(np.dot(a2,theta2))\n",
    "        d3=sig-y\n",
    "        d2=np.dot(d3,theta2.T)\n",
    "        z2=self.sigmoidGradient(np.concatenate((one,np.dot(a1,theta1)),1))\n",
    "        d2=np.multiply(d2,z2)\n",
    "        delta1=np.dot(a1.T,d2[:,1:])\n",
    "        delta2=np.dot(a2.T,d3)\n",
    "        one=np.ones((1,hidden_layer_size))\n",
    "        theta1=np.concatenate((one,theta1[1:,:]),0)\n",
    "        one=np.ones((1,output_layer_size))\n",
    "        theta2=np.concatenate((one,theta2[1:,:]),0)\n",
    "        t1_grad=(1/m)*delta1+(lamb/m)*theta1\n",
    "        t2_grad=(1/m)*delta2+(lamb/m)*theta2\n",
    "        grad=self.theta_flatten([t1_grad,t2_grad])\n",
    "        #print(grad.shape)\n",
    "        return grad\n",
    "    def test(self,X):\n",
    "        if self.feature_scaling == 'normalize':\n",
    "            X=normalize(X)\n",
    "        elif self.feature_scaling == 'standardize':\n",
    "            X=standardize(X)\n",
    "        m,n=X.shape\n",
    "        theta1,theta2=self.theta_unflatten(self.theta,self.input_layer_size,self.hidden_layer_size,self.output_layer_size)\n",
    "        one=np.ones((m,1))\n",
    "        a1=np.concatenate((one,X),1)\n",
    "        a2=np.concatenate((one,self.sigmoid(np.dot(a1,theta1))),1)\n",
    "        sig=self.sigmoid(np.dot(a2,theta2))\n",
    "        sig[sig>0.5]=1\n",
    "        sig[sig<0.5]=0\n",
    "        #print((sig[sig==y].shape[0]/X.shape[0])/output_layer_size)\n",
    "        return sig\n",
    "    def train(self,X,y):\n",
    "        if self.feature_scaling == 'normalize':\n",
    "            X=normalize(X)\n",
    "        elif self.feature_scaling == 'standardize':\n",
    "            X=standardize(X)\n",
    "        m,n=X.shape\n",
    "        self.input_layer_size=n\n",
    "        self.hidden_layer_size=n+1\n",
    "        m,n=y.shape\n",
    "        self.output_layer_size=n\n",
    "        self.theta=self.initialise_thetas(self.input_layer_size,self.hidden_layer_size,self.output_layer_size)\n",
    "        arg=X,y,self.input_layer_size,self.hidden_layer_size,self.output_layer_size,self.lamb\n",
    "        self.theta=fmin_cg(self.costFunction,x0=self.theta, fprime= self.nnGradient,args=arg)\n",
    "        print('Training complete')\n",
    "        print(self.theta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn=NeuralNetwork(feature_scaling=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1=np.random.randint(1,high=20,size=(200,2))\n",
    "X2=np.random.randint(17,high=32,size=(200,2))\n",
    "X=np.concatenate((X1,X2),axis=0)\n",
    "y1=np.zeros((200,1))\n",
    "y2=np.ones((200,1))\n",
    "y=np.concatenate((y1,y2),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn.train(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.concatenate((nn.test(X_test),y_test),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
