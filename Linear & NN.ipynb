{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models\n",
    "This file is for test and creation of linear/logistic regression models\n",
    "1. Linear_regression built - testing to be done\n",
    "2. Logistic_regression to be built as a class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " feature scaling occurs across categories i.e. we're training a model\n",
    " to recognise dogs, we normalise across:\n",
    " tail length, leg length, nose-pointy-outness\n",
    "                  normalisation (x-xmin)/(xmax-xmin)\n",
    "                 standardisation (X-Xsample_mean)/Xsample_std\n",
    "\"\"\"\n",
    "def normalize(X):\n",
    "    # normalize across features\n",
    "    m,n = X.shape\n",
    "    for col in range(n):\n",
    "        X[:,col] = ((X[:,col]-np.min(X[:,col]))/(np.max(X[:,col])-np.min(X[:,col])))\n",
    "    return X\n",
    "def standardize(X):\n",
    "    # standardize across features\n",
    "    m,n = X.shape\n",
    "    for col in range(n):\n",
    "        X[:,col] = (X[:,col]-np.mean(X[:,col]))/np.std(X[:,col])\n",
    "    return X\n",
    "# Creation of linear regression class:\n",
    "class Linear_regression:\n",
    "    \"\"\"Linear regression model\n",
    "    Parameters\n",
    "    -----------\n",
    "    alpha : float, default 0.01\n",
    "        learning rate of gradient descent\n",
    "    degree_accuracy : float, default 0.05\n",
    "        degree of accuracy that linear\n",
    "        regression is looking for during \n",
    "        gradient descent\n",
    "    feature_scaling : string, default None\n",
    "        options: None,'normalize'\n",
    "        \"\"\"\n",
    "    def __init__(self,alpha=0.01, degree_accuracy=0.05,feature_scaling=None):\n",
    "        self.alpha = alpha\n",
    "        self.degree_accuracy = degree_accuracy\n",
    "        self.thetas_set = False\n",
    "        self.feature_scaling_options = [None,'normalize']\n",
    "        assert feature_scaling in self.feature_scaling_options,\"no such feature scaling option\"\n",
    "        self.feature_scaling = feature_scaling\n",
    "        self.theta = np.array([])\n",
    "    def generate_theta(self,X):\n",
    "        self.theta = np.random.rand(X.shape[1],1)\n",
    "    def add_intercept(self,X):\n",
    "        return np.concatenate((X,np.ones((X.shape[0],1))),axis=1)\n",
    "    def computeCost(self,X,y,theta):\n",
    "        # Cost function\n",
    "        m,n=X.shape\n",
    "        J=0\n",
    "        X=(np.dot(X,theta))\n",
    "        J=sum(np.power(X-y,2))\n",
    "        J*=(1.0/(2.0*m))\n",
    "        return J\n",
    "    def coefficients(self):\n",
    "        # prints coefficients\n",
    "        print(self.theta[:-1,:])\n",
    "    def intercept(self):\n",
    "        # print intercept\n",
    "        print(self.theta[-1,:])\n",
    "    def fit(self,X,y):\n",
    "        \"\"\"fits model\n",
    "        Parameters\n",
    "        --------------\n",
    "        X : numpy array\n",
    "            Array should be independent variables.\n",
    "            Shape must be m * n, where m is cases\n",
    "            and n is features\n",
    "        y : numpy array\n",
    "            Array should be dependent variable\n",
    "            Shape should be m * 1, where m is cases\n",
    "        Returns\n",
    "        --------------\n",
    "        theta : numpy array\n",
    "            These are the coefficients of the linear\n",
    "            regression. This will be stored in the \n",
    "            model for use in predict\"\"\"\n",
    "        \n",
    "        alpha=self.alpha\n",
    "        degree_accuracy=self.degree_accuracy\n",
    "        if self.thetas_set == False:\n",
    "            if self.feature_scaling == 'normalize':\n",
    "                X=normalize(X)\n",
    "            X = self.add_intercept(X)\n",
    "            self.generate_theta(X)\n",
    "            self.thetas_set = True\n",
    "        theta = self.theta\n",
    "        h=0\n",
    "        m,n=X.shape\n",
    "        J_history=[]\n",
    "        J=999.0\n",
    "        iterations=0\n",
    "        print(theta)\n",
    "        while J>=degree_accuracy:\n",
    "            h=np.dot(X,theta)\n",
    "            #print('h1: {0}'.format(h))\n",
    "            h=np.dot(X.T,h-y)/float(m)\n",
    "            #print('h2: {0}'.format(h))\n",
    "            theta=theta-(h*alpha)\n",
    "            #print(theta)\n",
    "            J=self.computeCost(X,y,theta)\n",
    "            J_history.append(J)\n",
    "            iterations+=1\n",
    "            if iterations > 2 and J > J_history[-2]:\n",
    "                print('J increasing, reducing alpha to: {0}'.format(alpha/2))\n",
    "                self.generate_theta(X)\n",
    "                self.alpha = alpha/2\n",
    "                theta = self.fit(X,y)\n",
    "                return theta\n",
    "        #print(J_history)\n",
    "        print('Returned Thetas are: {0}'.format(theta))\n",
    "        print('degree of accuracy: %s' % J_history[-1])\n",
    "        print('number of iterations: %s' % iterations)\n",
    "        self.theta = theta\n",
    "        return theta\n",
    "    def predict(self,X):\n",
    "        \"\"\"Use this function to predict y\n",
    "        from a set of X data\n",
    "        Parameters\n",
    "        ------------\n",
    "        X : numpy array\n",
    "            independent variables, shaped\n",
    "            m * n, where m is case and n is \n",
    "            feature\n",
    "        \"\"\"\n",
    "        if self.feature_scaling == 'normalize':\n",
    "            X=normalize(X)\n",
    "        X = self.add_intercept(X)\n",
    "        return np.dot(X,self.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Step 1 generate FAKE data\n",
    "# These are the test thetas being used to generate fake data\n",
    "theta0=13\n",
    "theta1=21\n",
    "X=np.random.randint(1,high=80,size=(90,1))\n",
    "X=X.reshape(len(X),1)\n",
    "#one=np.ones(X.shape)\n",
    "#X=np.concatenate((one,X),axis=1)\n",
    "y=np.array([[theta0+X[i,0]*theta1] for i in range(len(X))])\n",
    "\n",
    "# Step 2 - create and fit linear regression model\n",
    "lr=Linear_regression(feature_scaling=None)\n",
    "lr.fit(X,y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.intercept()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression - base functions:\n",
    "\n",
    "def sigmoid(z):\n",
    "    return np.array(1/(1.0 + np.exp(-z)),dtype=float)\n",
    "def cost_function(theta,X,y):\n",
    "    m,n = X.shape\n",
    "    theta = theta.reshape(n,1)\n",
    "    z=np.dot(X,theta)\n",
    "    h=sigmoid(z)\n",
    "    J = (-1.0/float(m))\n",
    "    p1=np.log(h)\n",
    "    p1 = np.dot(p1.T,y)\n",
    "    p2=np.log(1.0-h)\n",
    "    p2=np.dot(p2.T,(1.0-y))\n",
    "    print(p2)\n",
    "    J=(-1/float(m))*(p1+p2)\n",
    "    return J.flatten()\n",
    "def gradient(theta,X,y):\n",
    "    m,n = X.shape\n",
    "    theta = theta.reshape(n,1)\n",
    "    z=np.dot(X,theta)\n",
    "    h=sigmoid(z)\n",
    "    grad = (1/float(m))*np.dot(X.T,(h-y))\n",
    "    return grad.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X=iris.data[:,:2]\n",
    "y=iris.target\n",
    "theta=np.random.rand(X.shape[1])\n",
    "X=X[y<2,:]\n",
    "y=y[y<2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "X=normalize(X)\n",
    "#cost_function(theta,X,y)\n",
    "#gradient(theta,X,y)\n",
    "res=minimize(cost_function,theta,args=(X,y),jac=gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "m=97\n",
    "alpha=0.01\n",
    "test_e=np.zeros((m,1))\n",
    "test_e+=np.exp(1)\n",
    "theta=np.zeros((3,1))\n",
    "y=np.zeros((m,1))\n",
    "one=np.ones((m,1))\n",
    "y+=0.01\n",
    "X=np.ones((m,3))\n",
    "for x in range(len(y)):\n",
    "    if x<45:\n",
    "        y[x]=0\n",
    "        X[x,1]= random.randint(1,6)\n",
    "        X[x,2]= random.random()*2\n",
    "        \n",
    "    else:\n",
    "        y[x]=1\n",
    "        X[x,1]= random.randint(5,11)\n",
    "        X[x,2]= 2+random.random()*7\n",
    "def cost_function(theta,X,y):\n",
    "    m,n=X.shape\n",
    "    theta = theta.reshape(n,1)\n",
    "    j=np.array((-1/float(m))*(np.dot(y.T,np.log(g(X,theta)))-np.dot((one-y).T,np.log(1-g(X,theta)))),dtype=float)\n",
    "    return np.absolute(j)\n",
    "def gradient_descent(alpha,theta,X,y,m):\n",
    "    theta=theta-(alpha/m)*np.dot(X.T,g(X,theta)-y)\n",
    "    return theta\n",
    "def gradient(theta,X,y):\n",
    "    m,n = X.shape\n",
    "    theta = theta.reshape(n,1)\n",
    "    return np.dot(X.T,g(X,theta)-y).flatten()\n",
    "def g(X,theta):\n",
    "    return 1/(1+np.power(test_e,-np.dot(X,theta)))\n",
    "for x in range(1000):\n",
    "    theta = gradient_descent(alpha,theta,X,y,m)\n",
    "#print(cost_function(X,y,theta))\n",
    "print(theta)\n",
    "#print(test_e)\n",
    "print(g(X,theta))\n",
    "#gradient_descent(alpha,theta,X,y,m)\n",
    "cost_function(theta,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_cg #fmin_cg to train neural network\n",
    "result=fmin_cg(cost_function,x0=np.random.randn(3,),args=(X,y),fprime=gradient)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_cg #fmin_cg to train neural network\n",
    "\n",
    "class Logistic_regression:\n",
    "    def __init__(self,alpha=0.01):\n",
    "        self.alpha=alpha\n",
    "        self.theta=np.array([])\n",
    "    def g(self,X,theta):\n",
    "        return 1/(1+np.power(test_e,-np.dot(X,theta)))\n",
    "    def cost_function(self,theta,X,y):\n",
    "        m,n=X.shape\n",
    "        test_e=np.zeros((m,1))\n",
    "        test_e+=np.exp(1)\n",
    "        one=np.ones((m,1))\n",
    "        theta = theta.reshape(n,1)\n",
    "        j=np.array((-1/float(m))*(np.dot(y.T,np.log(self.g(X,theta)))-np.dot((one-y).T,np.log(1-self.g(X,theta)))),dtype=float)\n",
    "        return np.absolute(j)\n",
    "    def gradient(self,theta,X,y):\n",
    "        m,n = X.shape\n",
    "        theta = theta.reshape(n,1)\n",
    "        return np.dot(X.T,self.g(X,theta)-y).flatten()\n",
    "    def fit(self,X,y):\n",
    "        m,n = X.shape\n",
    "        self.theta = np.random.randn(n)\n",
    "        theta = self.theta\n",
    "        self.theta = fmin_cg(self.cost_function,x0=theta,args=(X,y),fprime=self.gradient)\n",
    "        print('fitting complete')\n",
    "    def predict(self,X):\n",
    "        m,n = X.shape\n",
    "        theta = self.theta.reshape(n,1)\n",
    "        sig = self.g(X,theta)\n",
    "        sig[sig<0.5]=0\n",
    "        sig[sig>0.5]=1\n",
    "        return sig\n",
    "    def accuracy(self,X,y):\n",
    "        m,n = X.shape\n",
    "        theta = self.theta.reshape(n,1)\n",
    "        sig = self.g(X,theta)\n",
    "        sig[sig<0.5]=0\n",
    "        sig[sig>0.5]=1\n",
    "        return len(y[y==sig])/float(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=Logistic_regression(alpha=0.05)\n",
    "lr.fit(X,y)\n",
    "lr.predict(X)\n",
    "lr.accuracy(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_cg #fmin_cg to train neural network\n",
    "class NeuralNetwork:\n",
    "    def __init__(self,hidden_layer_size=20,epsilon=0.12,lamb=1,feature_scaling=None):\n",
    "        self.hidden_layer_size=hidden_layer_size\n",
    "        self.epsilon=epsilon\n",
    "        self.lamb=lamb\n",
    "        self.X=np.array([[]])\n",
    "        self.y=np.array([[]])\n",
    "        self.theta=np.array([[]])\n",
    "        self.input_layer_size=5\n",
    "        self.output_layer_size=1\n",
    "        self.feature_scaling_options = [None,'normalize','standardize']\n",
    "        assert feature_scaling in self.feature_scaling_options,\"no such feature scaling option\"\n",
    "        self.feature_scaling=feature_scaling\n",
    "    def sigmoid(self,z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    def sigmoidGradient(self,z):\n",
    "        return np.multiply(self.sigmoid(z),(1-self.sigmoid(z)))\n",
    "    def initialise_thetas(self,input_layer_size,hidden_layer_size,output_layer_size):\n",
    "        theta1=np.random.rand(input_layer_size+1,hidden_layer_size)\n",
    "        theta2=np.random.rand(hidden_layer_size+1,output_layer_size)\n",
    "        theta=np.array([theta1,theta2])\n",
    "        theta=self.theta_flatten(theta)*2*self.epsilon-self.epsilon\n",
    "        return theta\n",
    "    def theta_flatten(self,theta):\n",
    "        theta_t=theta[:]\n",
    "        theta=np.array([])\n",
    "        #fmin_cg requires a gradient to be (m,0) dimensions\n",
    "        for x in theta_t:\n",
    "            theta=np.concatenate((theta,x.flatten()),0)\n",
    "        #theta=theta.reshape(len(theta),0)\n",
    "        #print(theta.dtype)\n",
    "        return theta\n",
    "    def theta_unflatten(self,theta,input_layer_size,hidden_layer_size,output_layer_size):\n",
    "        theta1=theta[:(input_layer_size+1)*hidden_layer_size].reshape((input_layer_size+1),hidden_layer_size)\n",
    "        theta2=theta[(input_layer_size+1)*hidden_layer_size:].reshape(hidden_layer_size+1,output_layer_size)\n",
    "        return theta1, theta2\n",
    "    def costFunction(self,theta,X,y,input_layer_size,hidden_layer_size,output_layer_size,lamb):\n",
    "        m,n=X.shape\n",
    "        theta1,theta2=self.theta_unflatten(theta,input_layer_size,hidden_layer_size,output_layer_size)\n",
    "        one=np.ones((m,1))\n",
    "        a1=np.concatenate((one,X),1)\n",
    "        a2=np.concatenate((one,self.sigmoid(np.dot(a1,theta1))),1)\n",
    "        sig=self.sigmoid(np.dot(a2,theta2))\n",
    "        cost=np.multiply(-y,np.log(sig))-np.multiply((1-y),np.log(1-sig))\n",
    "        theta1_bias=theta1[1:,:]\n",
    "        theta2_bias=theta2[1:,:]\n",
    "        J=(1/m)*sum(sum(cost))+(lamb/(2*m))*(sum(sum(np.square(theta1_bias)))+sum(sum(np.square(theta2_bias))))\n",
    "        return J\n",
    "    def nnGradient(self,theta,X,y,input_layer_size,hidden_layer_size,output_layer_size,lamb):\n",
    "        m,n=X.shape\n",
    "        theta1,theta2=self.theta_unflatten(theta,input_layer_size,hidden_layer_size,output_layer_size)\n",
    "        one=np.ones((m,1))\n",
    "        a1=np.concatenate((one,X),1)\n",
    "        a2=np.concatenate((one,self.sigmoid(np.dot(a1,theta1))),1)\n",
    "        sig=self.sigmoid(np.dot(a2,theta2))\n",
    "        d3=sig-y\n",
    "        d2=np.dot(d3,theta2.T)\n",
    "        z2=self.sigmoidGradient(np.concatenate((one,np.dot(a1,theta1)),1))\n",
    "        d2=np.multiply(d2,z2)\n",
    "        delta1=np.dot(a1.T,d2[:,1:])\n",
    "        delta2=np.dot(a2.T,d3)\n",
    "        one=np.ones((1,hidden_layer_size))\n",
    "        theta1=np.concatenate((one,theta1[1:,:]),0)\n",
    "        one=np.ones((1,output_layer_size))\n",
    "        theta2=np.concatenate((one,theta2[1:,:]),0)\n",
    "        t1_grad=(1/m)*delta1+(lamb/m)*theta1\n",
    "        t2_grad=(1/m)*delta2+(lamb/m)*theta2\n",
    "        grad=self.theta_flatten([t1_grad,t2_grad])\n",
    "        #print(grad.shape)\n",
    "        return grad\n",
    "    def test(self,X):\n",
    "        if self.feature_scaling == 'normalize':\n",
    "            X=normalize(X)\n",
    "        elif self.feature_scaling == 'standardize':\n",
    "            X=standardize(X)\n",
    "        m,n=X.shape\n",
    "        theta1,theta2=self.theta_unflatten(self.theta,self.input_layer_size,self.hidden_layer_size,self.output_layer_size)\n",
    "        one=np.ones((m,1))\n",
    "        a1=np.concatenate((one,X),1)\n",
    "        a2=np.concatenate((one,self.sigmoid(np.dot(a1,theta1))),1)\n",
    "        sig=self.sigmoid(np.dot(a2,theta2))\n",
    "        sig[sig>0.5]=1\n",
    "        sig[sig<0.5]=0\n",
    "        #print((sig[sig==y].shape[0]/X.shape[0])/output_layer_size)\n",
    "        return sig\n",
    "    def train(self,X,y):\n",
    "        if self.feature_scaling == 'normalize':\n",
    "            X=normalize(X)\n",
    "        elif self.feature_scaling == 'standardize':\n",
    "            X=standardize(X)\n",
    "        m,n=X.shape\n",
    "        self.input_layer_size=n\n",
    "        self.hidden_layer_size=n+1\n",
    "        m,n=y.shape\n",
    "        self.output_layer_size=n\n",
    "        self.theta=self.initialise_thetas(self.input_layer_size,self.hidden_layer_size,self.output_layer_size)\n",
    "        arg=X,y,self.input_layer_size,self.hidden_layer_size,self.output_layer_size,self.lamb\n",
    "        self.theta=fmin_cg(self.costFunction,x0=self.theta, fprime= self.nnGradient,args=arg)\n",
    "        print('Training complete')\n",
    "        print(self.theta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn=NeuralNetwork(feature_scaling=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1=np.random.randint(1,high=20,size=(200,2))\n",
    "X2=np.random.randint(17,high=32,size=(200,2))\n",
    "X=np.concatenate((X1,X2),axis=0)\n",
    "y1=np.zeros((200,1))\n",
    "y2=np.ones((200,1))\n",
    "y=np.concatenate((y1,y2),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.train(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate((nn.test(X_test),y_test),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
