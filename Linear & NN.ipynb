{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Linear_regression:\n",
    "    def __init__(self,alpha=0.01, degree_accuracy=0.05):\n",
    "        self.alpha = alpha\n",
    "        self.degree_accuracy = degree_accuracy\n",
    "        self.thetas_set = False\n",
    "    def generate_theta(self,X):\n",
    "        self.theta = np.random.rand(X.shape[1],1)\n",
    "    #Below is the costfunction\n",
    "    def computeCost(self,X,y,theta):\n",
    "        m,n=X.shape\n",
    "        J=0\n",
    "        X=(np.dot(X,theta))\n",
    "        J=sum(np.power(X-y,2))\n",
    "        #print(J)\n",
    "        J*=(1.0/(2.0*m))\n",
    "        return J\n",
    "    def fit(self,X,y):\n",
    "        \n",
    "        alpha=self.alpha\n",
    "        degree_accuracy=self.degree_accuracy\n",
    "        if self.thetas_set = False:\n",
    "            self.generate_theta(X)\n",
    "            self.thetas_set = True\n",
    "        theta = self.theta\n",
    "        h=0\n",
    "        m,n=X.shape\n",
    "        J_history=[]\n",
    "        J=999.0\n",
    "        iterations=0\n",
    "        print(theta)\n",
    "        while J>=degree_accuracy:\n",
    "            h=np.dot(X,theta)\n",
    "            #print('h1: {0}'.format(h))\n",
    "            h=np.dot(X.T,h-y)/float(m)\n",
    "            #print('h2: {0}'.format(h))\n",
    "            theta=theta-(h*alpha)\n",
    "            #print(theta)\n",
    "            J=self.computeCost(X,y,theta)\n",
    "            J_history.append(J)\n",
    "            iterations+=1\n",
    "            if iterations > 2 and J > J_history[-2]:\n",
    "                print('J increasing, reducing alpha to: {0}'.format(alpha/2))\n",
    "                self.alpha = alpha/2\n",
    "                theta = self.fit(X,y)\n",
    "                return theta\n",
    "        #print(J_history)\n",
    "        print('Returned Thetas are: {0}'.format(theta))\n",
    "        print('degree of accuracy: %s' % J_history[-1])\n",
    "        print('number of iterations: %s' % iterations)\n",
    "        self.theta = theta\n",
    "        return theta\n",
    "    def predict(self,X):\n",
    "        return np.dot(X,self.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "theta0=13\n",
    "theta1=21\n",
    "X=np.arange(1,24)\n",
    "X=X.reshape(len(X),1)\n",
    "one=np.ones(X.shape)\n",
    "\n",
    "X=np.concatenate((one,X),axis=1)\n",
    "y=np.array([[theta0+X[i,1]*theta1] for i in range(len(X))])\n",
    "print(y.shape)\n",
    "lr=Linear_regression(alpha=0.01)\n",
    "lr.fit(X,y)\n",
    "#plt.plot(X[:,0],y)\n",
    "#plt.show()\n",
    "#plt.plot(X[:,0],lr.predict(X))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array([[0.0915497],[0.76109362]])\n",
    "h1=np.dot(X,np.array())\n",
    "np.dot(X.T,h1-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#thetas that we want it to return\n",
    "theta0=13\n",
    "theta1=21\n",
    "print('Original thetas are: %s & %s' % (theta0, theta1))\n",
    "X=np.arange(1,21)\n",
    "X=X.reshape(len(X),1)\n",
    "#one=np.ones(X.shape)\n",
    "\n",
    "#X=np.concatenate((one,X),axis=1)\n",
    "y=np.array([[X[i,0]*theta1] for i in range(len(X))])\n",
    "theta=np.random.rand(1,1)\n",
    "alpha=0.01\n",
    "degree_accuracy=0.0005\n",
    "\n",
    "#Below is the costfunction\n",
    "def computeCost(X,y,theta):\n",
    "    m,n=X.shape\n",
    "    J=0\n",
    "    #print(\"X shape: \")\n",
    "    #print(m,n)\n",
    "    X=(np.dot(X,theta))\n",
    "    #print(X-y)\n",
    "    J=sum(np.power(X-y,2))\n",
    "    #print(J)\n",
    "    J*=(1/(2.0*m))\n",
    "    #print(J)\n",
    "    return J\n",
    "\n",
    "#below function carries out gradient_descent\n",
    "#This depends on number of iterations\n",
    "def gradient_descent(X,y,theta,alpha,degree_accuracy):\n",
    "    h=0\n",
    "    m,n=X.shape\n",
    "    J_history=[]\n",
    "    J=999\n",
    "    iterations=0\n",
    "    print(theta)\n",
    "    while J>=degree_accuracy:\n",
    "        h=np.dot(X,theta)\n",
    "        h=np.dot(X.T,h-y)\n",
    "        print(h)\n",
    "        theta=theta-(h*alpha/m)\n",
    "        #print(theta)\n",
    "        J=computeCost(X,y,theta)\n",
    "        J_history.append(J)\n",
    "        iterations+=1\n",
    "    #print(J_history)\n",
    "    print('Returned Thetas are: %s ' % (theta[0,0]))\n",
    "    print('degree of accuracy: %s' % J_history[-1])\n",
    "    print('number of iterations: %s' % iterations)\n",
    "\n",
    "gradient_descent(X,y,theta,alpha,degree_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array([[1],[4],[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta=np.array([[2],[1]])\n",
    "computeCost(X,y,theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This is an attempt to knock up a neural network that will categorise cases\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import scipy.misc #Used to show matrix as an image\n",
    "import random #To pick random images to display\n",
    "from scipy.optimize import fmin_cg #fmin_cg to train neural network\n",
    "import itertools\n",
    "from scipy.special import expit #Vectorized sigmoid function\n",
    "\n",
    "cases=60\n",
    "input_layer_size = 20\n",
    "hidden_layer_size = 21\n",
    "output_layer_size = 3\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "def sigmoidGradient(z):\n",
    "    return np.multiply(sigmoid(z),(1-sigmoid(z)))\n",
    "#normalizes the mean for input into data\n",
    "#this is intended to calc sample mean/std\n",
    "def mean_normalization(X):\n",
    "    for i in range(X.shape[1]):\n",
    "        i_mean=np.sum(X[:,i])/X.shape[0]\n",
    "        #i_mean=np.mean(X[:,i],ddof=1)\n",
    "        i_std=np.std(X[:,i],ddof=1)\n",
    "        X[:,i]=(X[:,i]-i_mean)/i_std\n",
    "    return X\n",
    "#X & y generator:\n",
    "\n",
    "lamb=0.01\n",
    "#generates our fake data\n",
    "def genXy(cases,n):\n",
    "    X=np.random.rand(cases,input_layer_size)\n",
    "    X[:int(cases/3),:]+=np.array(np.random.random_integers(4, 10,(int(cases/3),n)),dtype=float)\n",
    "    X[int(cases/3):2*int(cases/3),:]+=np.array(np.random.random_integers(7, 14,(2*int(cases/3)-int(cases/3),n)),dtype=float)\n",
    "    X[2*int(cases/3):,:]+=np.array(np.random.random_integers(11, 17,(cases-2*int(cases/3),n)),dtype=float)\n",
    "    X=mean_normalization(X)\n",
    "\n",
    "    y=np.zeros((cases,3))\n",
    "    y[:int(cases/3),0]=1\n",
    "    y[int(cases/3):2*int(cases/3),1]=1\n",
    "    y[2*int(cases/3):,2]=1\n",
    "    return X,y\n",
    "X,y=genXy(cases,input_layer_size)\n",
    "def initialise_thetas(input_layer_size,hidden_layer_size,output_layer_size):\n",
    "    epsilon=0.12\n",
    "    theta1=np.random.rand(input_layer_size+1,hidden_layer_size)\n",
    "    theta2=np.random.rand(hidden_layer_size+1,output_layer_size)\n",
    "    theta=np.array([theta1,theta2])\n",
    "    theta=theta_flatten(theta)*2*epsilon-epsilon\n",
    "    return theta\n",
    "#flattens whatever is fed into it\n",
    "def theta_flatten(theta):\n",
    "    theta_t=theta[:]\n",
    "    theta=np.array([])\n",
    "    #fmin_cg requires a gradient to be (m,0) dimensions\n",
    "    for x in theta_t:\n",
    "        theta=np.concatenate((theta,x.flatten()),0)\n",
    "    #theta=theta.reshape(len(theta),0)\n",
    "    #print(theta.dtype)\n",
    "    return theta\n",
    "#unflatten into correct shape\n",
    "def theta_unflatten(theta,input_layer_size,hidden_layer_size,output_layer_size):\n",
    "    theta1=theta[:(input_layer_size+1)*hidden_layer_size].reshape((input_layer_size+1),hidden_layer_size)\n",
    "    theta2=theta[(input_layer_size+1)*hidden_layer_size:].reshape(hidden_layer_size+1,output_layer_size)\n",
    "    return theta1, theta2\n",
    "def costFunction(theta,X,y,input_layer_size,hidden_layer_size,output_layer_size,lamb):\n",
    "    m,n=X.shape\n",
    "    theta1,theta2=theta_unflatten(theta,input_layer_size,hidden_layer_size,output_layer_size)\n",
    "    one=np.ones((m,1))\n",
    "    a1=np.concatenate((one,X),1)\n",
    "    a2=np.concatenate((one,sigmoid(np.dot(a1,theta1))),1)\n",
    "    sig=sigmoid(np.dot(a2,theta2))\n",
    "\n",
    "    cost=np.multiply(-y,np.log(sig))-np.multiply((1-y),np.log(1-sig))\n",
    "    theta1_bias=theta1[1:,:]\n",
    "    theta2_bias=theta2[1:,:]\n",
    "    J=(1/float(m))*sum(sum(cost))+(lamb/(2*m))*(sum(sum(np.square(theta1_bias)))+sum(sum(np.square(theta2_bias))))\n",
    "    return J\n",
    "def nnGradient(theta,X,y,input_layer_size,hidden_layer_size,output_layer_size,lamb):\n",
    "    m,n=X.shape\n",
    "    theta1,theta2=theta_unflatten(theta,input_layer_size,hidden_layer_size,output_layer_size)\n",
    "    one=np.ones((m,1))\n",
    "    a1=np.concatenate((one,X),1)\n",
    "    a2=np.concatenate((one,sigmoid(np.dot(a1,theta1))),1)\n",
    "    sig=sigmoid(np.dot(a2,theta2))\n",
    "    d3=sig-y\n",
    "    d2=np.dot(d3,theta2.T)\n",
    "    z2=sigmoidGradient(np.concatenate((one,np.dot(a1,theta1)),1))\n",
    "    d2=np.multiply(d2,z2)\n",
    "    delta1=np.dot(a1.T,d2[:,1:])\n",
    "    delta2=np.dot(a2.T,d3)\n",
    "    one=np.ones((1,hidden_layer_size))\n",
    "    theta1=np.concatenate((one,theta1[1:,:]),0)\n",
    "    one=np.ones((1,output_layer_size))\n",
    "    theta2=np.concatenate((one,theta2[1:,:]),0)\n",
    "    t1_grad=(1.0/m)*delta1+(lamb/m)*theta1\n",
    "    t2_grad=(1.0/m)*delta2+(lamb/m)*theta2\n",
    "    grad=theta_flatten([t1_grad,t2_grad])\n",
    "    #print(grad.shape)\n",
    "    return grad\n",
    "#run initial code to set up theta\n",
    "theta=initialise_thetas(input_layer_size,hidden_layer_size,output_layer_size)\n",
    "\n",
    "#arguments stored for minimizing\n",
    "arg=X,y,input_layer_size,hidden_layer_size,output_layer_size,lamb\n",
    "res1=fmin_cg(costFunction,x0=theta, fprime= nnGradient,args=arg)\n",
    "#output of model is used to test\n",
    "theta=res1\n",
    "#function produces output to be tested\n",
    "def tester(theta,X,y,input_layer_size,hidden_layer_size,output_layer_size,lamb):\n",
    "    m,n=X.shape\n",
    "    theta1,theta2=theta_unflatten(theta,input_layer_size,hidden_layer_size,output_layer_size)\n",
    "    one=np.ones((m,1))\n",
    "    a1=np.concatenate((one,X),1)\n",
    "    a2=np.concatenate((one,sigmoid(np.dot(a1,theta1))),1)\n",
    "    sig=sigmoid(np.dot(a2,theta2))\n",
    "    sig[sig>0.5]=1.0\n",
    "    sig[sig<0.5]=0.0\n",
    "    print((sig[sig==y].shape[0]/X.shape[0])/output_layer_size)\n",
    "    return np.concatenate((sig,y),1)\n",
    "#t=tester(theta,X,y,m,n,input_layer_size,hidden_layer_size,output_layer_size,lamb)\n",
    "#For this you want the result to return a Current function value below 1\n",
    "\n",
    "#bootstrap the data in an effort to create more trainable information to see if it makes a better model\n",
    "def bootstrap(X,y,degrees):\n",
    "    X_output=np.zeros((degrees*X.shape[0],X.shape[1]))\n",
    "    y_output=np.zeros((degrees*y.shape[0],y.shape[1]))\n",
    "    for i in range(degrees*X.shape[0]):\n",
    "        ran=random.randint(0,X.shape[0]-1)\n",
    "        X_output[i,:]=X[ran,:]\n",
    "        y_output[i,:]=y[ran,:]\n",
    "    return X_output,y_output\n",
    "\n",
    "\n",
    "#test the initial model results by regenerating fake data \n",
    "\n",
    "X,y=genXy(cases,input_layer_size)\n",
    "t=tester(theta,X,y,input_layer_size,hidden_layer_size,output_layer_size,lamb)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
