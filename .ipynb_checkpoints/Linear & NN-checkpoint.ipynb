{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models\n",
    "This file is for test and creation of linear/logistic regression models\n",
    "1. Linear_regression built - testing to be done\n",
    "2. Logistic_regression to be built as a class\n",
    "3. Neural_network - simple perceptron to be built as class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " feature scaling occurs across categories i.e. we're training a model\n",
    " to recognise dogs, we normalise across:\n",
    " tail length, leg length, nose-pointy-outness\n",
    "                  normalisation (x-xmin)/(xmax-xmin)\n",
    "                 standardisation (X-Xsample_mean)/Xsample_std\n",
    "\"\"\"\n",
    "def normalize(X):\n",
    "    # normalize across features\n",
    "    m,n = X.shape\n",
    "    for col in range(n):\n",
    "        X[:,col] = ((X[:,col]-np.min(X[:,col]))/(np.max(X[:,col])-np.min(X[:,col])))\n",
    "    return X\n",
    "def standardize(X):\n",
    "    # standardize across features\n",
    "    m,n = X.shape\n",
    "    for col in range(n):\n",
    "        X[:,col] = (X[:,col]-np.mean(X[:,col]))/np.std(X[:,col])\n",
    "    return X\n",
    "# Creation of linear regression class:\n",
    "class Linear_regression:\n",
    "    \"\"\"Linear regression model\n",
    "    Parameters\n",
    "    -----------\n",
    "    alpha : float, default 0.01\n",
    "        learning rate of gradient descent\n",
    "    degree_accuracy : float, default 0.05\n",
    "        degree of accuracy that linear\n",
    "        regression is looking for during \n",
    "        gradient descent\n",
    "    feature_scaling : string, default None\n",
    "        options: None,'normalize'\n",
    "        \"\"\"\n",
    "    def __init__(self,alpha=0.01, degree_accuracy=0.05,feature_scaling=None):\n",
    "        self.alpha = alpha\n",
    "        self.degree_accuracy = degree_accuracy\n",
    "        self.thetas_set = False\n",
    "        self.feature_scaling_options = [None,'normalize']\n",
    "        assert feature_scaling in self.feature_scaling_options,\"no such feature scaling option\"\n",
    "        self.feature_scaling = feature_scaling\n",
    "        self.theta = np.array([])\n",
    "    def generate_theta(self,X):\n",
    "        self.theta = np.random.rand(X.shape[1],1)\n",
    "    def add_intercept(self,X):\n",
    "        return np.concatenate((X,np.ones((X.shape[0],1))),axis=1)\n",
    "    def computeCost(self,X,y,theta):\n",
    "        # Cost function\n",
    "        m,n=X.shape\n",
    "        J=0\n",
    "        X=(np.dot(X,theta))\n",
    "        J=sum(np.power(X-y,2))\n",
    "        J*=(1.0/(2.0*m))\n",
    "        return J\n",
    "    def coefficients(self):\n",
    "        # prints coefficients\n",
    "        print(self.theta[:-1,:])\n",
    "    def intercept(self):\n",
    "        # print intercept\n",
    "        print(self.theta[-1,:])\n",
    "    def fit(self,X,y):\n",
    "        \"\"\"fits model\n",
    "        Parameters\n",
    "        --------------\n",
    "        X : numpy array\n",
    "            Array should be independent variables.\n",
    "            Shape must be m * n, where m is cases\n",
    "            and n is features\n",
    "        y : numpy array\n",
    "            Array should be dependent variable\n",
    "            Shape should be m * 1, where m is cases\n",
    "        Returns\n",
    "        --------------\n",
    "        theta : numpy array\n",
    "            These are the coefficients of the linear\n",
    "            regression. This will be stored in the \n",
    "            model for use in predict\"\"\"\n",
    "        \n",
    "        alpha=self.alpha\n",
    "        degree_accuracy=self.degree_accuracy\n",
    "        if self.thetas_set == False:\n",
    "            if self.feature_scaling == 'normalize':\n",
    "                X=normalize(X)\n",
    "            X = self.add_intercept(X)\n",
    "            self.generate_theta(X)\n",
    "            self.thetas_set = True\n",
    "        theta = self.theta\n",
    "        h=0\n",
    "        m,n=X.shape\n",
    "        J_history=[]\n",
    "        J=999.0\n",
    "        iterations=0\n",
    "        print(theta)\n",
    "        while J>=degree_accuracy:\n",
    "            h=np.dot(X,theta)\n",
    "            #print('h1: {0}'.format(h))\n",
    "            h=np.dot(X.T,h-y)/float(m)\n",
    "            #print('h2: {0}'.format(h))\n",
    "            theta=theta-(h*alpha)\n",
    "            #print(theta)\n",
    "            J=self.computeCost(X,y,theta)\n",
    "            J_history.append(J)\n",
    "            iterations+=1\n",
    "            if iterations > 2 and J > J_history[-2]:\n",
    "                print('J increasing, reducing alpha to: {0}'.format(alpha/2))\n",
    "                self.generate_theta(X)\n",
    "                self.alpha = alpha/2\n",
    "                theta = self.fit(X,y)\n",
    "                return theta\n",
    "        #print(J_history)\n",
    "        print('Returned Thetas are: {0}'.format(theta))\n",
    "        print('degree of accuracy: %s' % J_history[-1])\n",
    "        print('number of iterations: %s' % iterations)\n",
    "        self.theta = theta\n",
    "        return theta\n",
    "    def predict(self,X):\n",
    "        \"\"\"Use this function to predict y\n",
    "        from a set of X data\n",
    "        Parameters\n",
    "        ------------\n",
    "        X : numpy array\n",
    "            independent variables, shaped\n",
    "            m * n, where m is case and n is \n",
    "            feature\n",
    "        \"\"\"\n",
    "        if self.feature_scaling == 'normalize':\n",
    "            X=normalize(X)\n",
    "        X = self.add_intercept(X)\n",
    "        return np.dot(X,self.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.34082353]\n",
      " [0.65065563]]\n",
      "J increasing, reducing alpha to: 0.005\n",
      "[[0.16133195]\n",
      " [0.34896986]]\n",
      "J increasing, reducing alpha to: 0.0025\n",
      "[[0.21451453]\n",
      " [0.5643427 ]]\n",
      "J increasing, reducing alpha to: 0.00125\n",
      "[[0.54974239]\n",
      " [0.60002906]]\n",
      "J increasing, reducing alpha to: 0.000625\n",
      "[[0.207827  ]\n",
      " [0.06004075]]\n",
      "Returned Thetas are: [[21.01362283]\n",
      " [12.34994406]]\n",
      "degree of accuracy: [0.04998874]\n",
      "number of iterations: 20000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[21.01362283],\n",
       "       [12.34994406]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Step 1 generate FAKE data\n",
    "# These are the test thetas being used to generate fake data\n",
    "theta0=13\n",
    "theta1=21\n",
    "X=np.random.randint(1,high=80,size=(90,1))\n",
    "X=X.reshape(len(X),1)\n",
    "#one=np.ones(X.shape)\n",
    "#X=np.concatenate((one,X),axis=1)\n",
    "y=np.array([[theta0+X[i,0]*theta1] for i in range(len(X))])\n",
    "\n",
    "# Step 2 - create and fit linear regression model\n",
    "lr=Linear_regression(feature_scaling=None)\n",
    "lr.fit(X,y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.34994406]\n"
     ]
    }
   ],
   "source": [
    "lr.intercept()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_cg #fmin_cg to train neural network\n",
    "class NeuralNetwork:\n",
    "    def __init__(self,hidden_layer_size=20,epsilon=0.12,lamb=1,feature_scaling=None):\n",
    "        self.hidden_layer_size=hidden_layer_size\n",
    "        self.epsilon=epsilon\n",
    "        self.lamb=lamb\n",
    "        self.X=np.array([[]])\n",
    "        self.y=np.array([[]])\n",
    "        self.theta=np.array([[]])\n",
    "        self.input_layer_size=5\n",
    "        self.output_layer_size=1\n",
    "        self.feature_scaling_options = [None,'normalize','standardize']\n",
    "        assert feature_scaling in self.feature_scaling_options,\"no such feature scaling option\"\n",
    "        self.feature_scaling=feature_scaling\n",
    "    def sigmoid(self,z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    def sigmoidGradient(self,z):\n",
    "        return np.multiply(self.sigmoid(z),(1-self.sigmoid(z)))\n",
    "    def initialise_thetas(self,input_layer_size,hidden_layer_size,output_layer_size):\n",
    "        theta1=np.random.rand(input_layer_size+1,hidden_layer_size)\n",
    "        theta2=np.random.rand(hidden_layer_size+1,output_layer_size)\n",
    "        theta=np.array([theta1,theta2])\n",
    "        theta=self.theta_flatten(theta)*2*self.epsilon-self.epsilon\n",
    "        return theta\n",
    "    def theta_flatten(self,theta):\n",
    "        theta_t=theta[:]\n",
    "        theta=np.array([])\n",
    "        #fmin_cg requires a gradient to be (m,0) dimensions\n",
    "        for x in theta_t:\n",
    "            theta=np.concatenate((theta,x.flatten()),0)\n",
    "        #theta=theta.reshape(len(theta),0)\n",
    "        #print(theta.dtype)\n",
    "        return theta\n",
    "    def theta_unflatten(self,theta,input_layer_size,hidden_layer_size,output_layer_size):\n",
    "        theta1=theta[:(input_layer_size+1)*hidden_layer_size].reshape((input_layer_size+1),hidden_layer_size)\n",
    "        theta2=theta[(input_layer_size+1)*hidden_layer_size:].reshape(hidden_layer_size+1,output_layer_size)\n",
    "        return theta1, theta2\n",
    "    def costFunction(self,theta,X,y,input_layer_size,hidden_layer_size,output_layer_size,lamb):\n",
    "        m,n=X.shape\n",
    "        theta1,theta2=self.theta_unflatten(theta,input_layer_size,hidden_layer_size,output_layer_size)\n",
    "        one=np.ones((m,1))\n",
    "        a1=np.concatenate((one,X),1)\n",
    "        a2=np.concatenate((one,self.sigmoid(np.dot(a1,theta1))),1)\n",
    "        sig=self.sigmoid(np.dot(a2,theta2))\n",
    "        cost=np.multiply(-y,np.log(sig))-np.multiply((1-y),np.log(1-sig))\n",
    "        theta1_bias=theta1[1:,:]\n",
    "        theta2_bias=theta2[1:,:]\n",
    "        J=(1/m)*sum(sum(cost))+(lamb/(2*m))*(sum(sum(np.square(theta1_bias)))+sum(sum(np.square(theta2_bias))))\n",
    "        return J\n",
    "    def nnGradient(self,theta,X,y,input_layer_size,hidden_layer_size,output_layer_size,lamb):\n",
    "        m,n=X.shape\n",
    "        theta1,theta2=self.theta_unflatten(theta,input_layer_size,hidden_layer_size,output_layer_size)\n",
    "        one=np.ones((m,1))\n",
    "        a1=np.concatenate((one,X),1)\n",
    "        a2=np.concatenate((one,self.sigmoid(np.dot(a1,theta1))),1)\n",
    "        sig=self.sigmoid(np.dot(a2,theta2))\n",
    "        d3=sig-y\n",
    "        d2=np.dot(d3,theta2.T)\n",
    "        z2=self.sigmoidGradient(np.concatenate((one,np.dot(a1,theta1)),1))\n",
    "        d2=np.multiply(d2,z2)\n",
    "        delta1=np.dot(a1.T,d2[:,1:])\n",
    "        delta2=np.dot(a2.T,d3)\n",
    "        one=np.ones((1,hidden_layer_size))\n",
    "        theta1=np.concatenate((one,theta1[1:,:]),0)\n",
    "        one=np.ones((1,output_layer_size))\n",
    "        theta2=np.concatenate((one,theta2[1:,:]),0)\n",
    "        t1_grad=(1/m)*delta1+(lamb/m)*theta1\n",
    "        t2_grad=(1/m)*delta2+(lamb/m)*theta2\n",
    "        grad=self.theta_flatten([t1_grad,t2_grad])\n",
    "        #print(grad.shape)\n",
    "        return grad\n",
    "    def test(self,X):\n",
    "        if self.feature_scaling == 'normalize':\n",
    "            X=normalize(X)\n",
    "        elif self.feature_scaling == 'standardize':\n",
    "            X=standardize(X)\n",
    "        m,n=X.shape\n",
    "        theta1,theta2=self.theta_unflatten(self.theta,self.input_layer_size,self.hidden_layer_size,self.output_layer_size)\n",
    "        one=np.ones((m,1))\n",
    "        a1=np.concatenate((one,X),1)\n",
    "        a2=np.concatenate((one,self.sigmoid(np.dot(a1,theta1))),1)\n",
    "        sig=self.sigmoid(np.dot(a2,theta2))\n",
    "        sig[sig>0.5]=1\n",
    "        sig[sig<0.5]=0\n",
    "        #print((sig[sig==y].shape[0]/X.shape[0])/output_layer_size)\n",
    "        return sig\n",
    "    def train(self,X,y):\n",
    "        if self.feature_scaling == 'normalize':\n",
    "            X=normalize(X)\n",
    "        elif self.feature_scaling == 'standardize':\n",
    "            X=standardize(X)\n",
    "        m,n=X.shape\n",
    "        self.input_layer_size=n\n",
    "        self.hidden_layer_size=n+1\n",
    "        m,n=y.shape\n",
    "        self.output_layer_size=n\n",
    "        self.theta=self.initialise_thetas(self.input_layer_size,self.hidden_layer_size,self.output_layer_size)\n",
    "        arg=X,y,self.input_layer_size,self.hidden_layer_size,self.output_layer_size,self.lamb\n",
    "        self.theta=fmin_cg(self.costFunction,x0=self.theta, fprime= self.nnGradient,args=arg)\n",
    "        print('Training complete')\n",
    "        print(self.theta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn=NeuralNetwork(feature_scaling=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X1=np.random.randint(1,high=20,size=(200,2))\n",
    "X2=np.random.randint(17,high=32,size=(200,2))\n",
    "X=np.concatenate((X1,X2),axis=0)\n",
    "y1=np.zeros((200,1))\n",
    "y2=np.ones((200,1))\n",
    "y=np.concatenate((y1,y2),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.163121\n",
      "         Iterations: 146\n",
      "         Function evaluations: 385\n",
      "         Gradient evaluations: 372\n",
      "Training complete\n",
      "(13,)\n"
     ]
    }
   ],
   "source": [
    "nn.train(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [0., 0.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((nn.test(X_test),y_test),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
